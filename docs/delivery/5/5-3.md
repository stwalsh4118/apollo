# 5-3: Research Skill Prompt

[Back to task list](./tasks.md)

## Description

Create the research skill prompt file at `skills/research.md` that drives Claude Code research sessions through the structured 4-pass pipeline. This file is loaded via `--system-prompt-file` when the Go orchestrator spawns research sessions. It is the "instruction manual" for the research agent.

This is the most complex task in PBI 5 — the prompt must be comprehensive enough to produce consistent, high-quality curriculum output across diverse topics.

## Status History

| Timestamp | Event Type | From Status | To Status | Details | User |
|-----------|------------|-------------|-----------|---------|------|
| 2026-02-19 07:11:45 | Created | N/A | Proposed | Task file created | AI_Agent |
| 2026-02-19 07:25:19 | Status Change | Agreed | InProgress | Started implementation | AI_Agent |
| 2026-02-19 07:28:36 | Status Change | InProgress | Review | Implementation complete, internal AI review passed | AI_Agent |
| 2026-02-19 07:58:47 | Status Change | Review | Done | Task completed and verified | AI_Agent |

## Requirements

1. **Pass 1 (Survey)** instructions per PRD 6.1:
   - Broad web search for the topic (official docs, Wikipedia, guides)
   - Identify scope and major areas/subtopics
   - Check `knowledge_pool_summary.json` for existing topics/concepts (Read tool)
   - Determine if topic splitting is needed (>8 modules threshold per PRD 5.4)
   - Output: proposed module structure, preliminary prerequisite list
   - Include topic splitting rules: if exceeding limit, return a split proposal instead of full curriculum

2. **Pass 2 (Deep Dive)** instructions per PRD 6.1:
   - Per-module focused research using multiple search queries per module
   - Official documentation, tutorials, community guides, best practices
   - Identify key concepts within each module
   - Mark concepts_taught vs concepts_referenced (referencing existing pool concepts)
   - Output: draft lessons with content sections, concepts, examples

3. **Pass 3 (Exercises & Assessment)** instructions per PRD 6.1:
   - Generate exercises per lesson using appropriate types from the 7-type spectrum (PRD 6.2)
   - Exercise type selection guidance: command (CLI tools), configuration (server setup), exploration (GUI tools), build (programming), troubleshooting (debugging), scenario (architecture), thought_experiment (theory)
   - Each exercise must include: title, instructions, success_criteria, hints, type, environment
   - Generate review questions testing understanding (not just recall)
   - Generate module-level assessments with conceptual and practical questions
   - Cross-reference concepts with existing knowledge pool

4. **Pass 4 (Self-Review)** checklist per PRD 6.1:
   - Are all learning objectives covered by lessons?
   - Does every lesson teach or reference at least one concept?
   - Are flashcard questions testing understanding, not just terminology?
   - Are exercises actionable (not vague "try this out")?
   - Do prerequisite classifications make sense?
   - Is the difficulty progression reasonable (foundational → advanced)?
   - Fix issues found during self-review before returning final output

5. **Prerequisite classification rules** per PRD 5.3:
   - Essential: auto-researched, can't learn parent without this
   - Helpful: improves understanding, not blocking, user opt-in
   - Deep background: academic/theoretical, for the genuinely curious
   - Each prerequisite must include topic_id and reason

6. **Topic splitting rules** per PRD 5.4:
   - If survey determines topic exceeds ~8 modules, propose a split
   - Each sub-topic becomes a coherent, standalone curriculum
   - Return a split proposal (sub-topic list with descriptions) instead of a full curriculum
   - Include `topic_size_limit` constraint awareness

7. **Knowledge pool integration**:
   - Instructions to Read `knowledge_pool_summary.json` during survey pass
   - Avoid duplicating concepts that already exist in the pool
   - Reference existing concepts via concepts_referenced
   - Define only new concepts via concepts_taught

8. **Output format awareness**:
   - Final pass output must match the curriculum JSON schema
   - Prompt should describe the expected structure clearly
   - Note that `--json-schema` will be applied on the final pass only

## Implementation Plan

1. Create `skills/` directory at project root
2. Write `skills/research.md` as a structured system prompt
3. Organize by pass with clear section headers
4. Include the self-review checklist as a numbered list
5. Include exercise type spectrum as a reference table
6. Include prerequisite classification rules as a reference table
7. Include topic splitting decision logic
8. Include knowledge pool context integration instructions
9. Describe the expected output structure inline (summary, not full schema — the schema itself is enforced by `--json-schema`)

## Test Plan

- Prompt file is well-structured markdown
- All 4 passes have clear, distinct instructions
- Self-review checklist from PRD 6.1 is present and complete (6 items)
- All 7 exercise types from PRD 6.2 are documented with selection guidance
- Prerequisite classification rules match PRD 5.3 (essential/helpful/deep_background)
- Topic splitting rules match PRD 5.4 (~8 module limit, split proposal format)
- Knowledge pool integration instructions reference `knowledge_pool_summary.json`
- Manual review for clarity and completeness

## Verification

- Prompt is well-structured markdown: PASS
- All 4 passes with clear, distinct instructions: PASS
- Self-review checklist from PRD 6.1 (6 items): PASS
- All 7 exercise types from PRD 6.2 with selection guidance: PASS
- Prerequisite classification rules match PRD 5.3: PASS
- Topic splitting rules match PRD 5.4 (~8 module limit): PASS
- Knowledge pool integration references knowledge_pool_summary.json: PASS
- Output format awareness (--json-schema on final pass): PASS
- Internal AI review: PASS (0 critical, 0 high, 3 medium deferred, 8 low)
- Medium deferred: split proposal format (deferred to PBI 6 orchestrator), type field implicit in table (stylistic), knowledge pool example (agent reads actual file)

## Files Modified

- `skills/research.md` (new)
